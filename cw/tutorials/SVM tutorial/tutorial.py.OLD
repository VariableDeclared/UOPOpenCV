
import json
import glob
import urllib.request
import argparse
import random
import os
import cv2 as cv
import sys
import numpy as np
import mahotas

def downloader(image_url, i):
    print("[INFO] Downloading: {}".format(image_url))
    file_name = str(i)
    full_file_name = str(file_name) + '.jpg'
    urllib.request.urlretrieve(image_url, full_file_name)

if __name__ == "__main__":
    a = argparse.ArgumentParser()
    a.add_argument("--json_file", help="path to json")
    a.add_argument("--dataset_path", help="path to the dataset")
    a.add_argument("--train-percentage", help="percentage of data for training")
    a.add_argument("--validation_percentage", help="percentage of data for validation")

    args = a.parse_args()

    if args.json_file is None and args.dataset_path is None and args.train_percentage is None \
        and args.validation_percentage is None:
        a.print_help()
        sys.exit()


    with open(args.json_file) as file1:
        lis = []
        for i in file1:
            lis.append(json.loads(i))

    train = float(int(args.train_percentage) / 100)
    test = float(int(args.validation_percentage) / 100)

    folder_names = []
    label_to_urls = {}


    for i in lis:
        if i['annotation']['labels'] == []:
            continue
        if i['annotation']['labels'][0] not in folder_names:
            folder_names.append(i['annotation']['labels'][0])
            label_to_urls[i['annotation']['labels'][0]] = [i['content']]
        else:
            label_to_urls[i['annotation']['labels'][0]].append(i['content'])


    print(label_to_urls.keys())

    os.mkdir(args.dataset_path)
    os.chdir(args.dataset_path)
    os.mkdir("train")
    os.mkdir("validation")

    os.chdir("train/")

    for i in label_to_urls.keys():
        os.mkdir(str(i))
        os.chdir(str(i))

        k = 0

        for j in label_to_urls[i][:round(0.8*(len(label_to_urls[i])))]:
            downloader(j, str(i)+str(k))
            k += 1
        os.chdir("../")
    print(os.getcwd())

    os.chdir("../validation")
    for i in label_to_urls.keys():
        os.mkdir(str(i))
        os.chdir(str(i))
        k = 0

        for j in label_to_urls[i][round(0.8*(len(label_to_urls[i]))):]:
            downloader(j, str(i) + str(k))
            k+=1
        os.chdir("../")

bins = 30

def fd_hu_moments(image):
    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
    feature = cv.HuMoments(cv.moments(image)).flatten()
    return feature

def fd_haralick(image): # convert image to grayscale
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
    # compute the haralick tecture feature vector
    haralick = mahotas.features.haralick(gray).mean(axis=0)
    return haralick
def fd_histogram(image, mask=None):
    # Convert the image to HSV color-space
    image = cv.cvtColor(image, cv.COLOR_BGR2HSV)
    image.size
    # compute the color histogram
    hist = cv2.calcHist([image], [0, 1, 2], mask, [bins, bins, bins], [0, 256, 0, 256, 0, 256])
    # normalize the histogram
    cv.normalize(hist, hist)
    hist.flatten()



global_feature = np.hstack([fd_histogram(image), fd_haralick(image), fd_hu_moments(image)])
scaler = MinMaxScaler(feature_range=(0, 1))
# Normalize the feature vectors
rescaled_features = scaler.fit_transform(global_features)



from sklearn.svm import SVC

clf = models.append(('SVM', SVC(random_state=9)))
prediction = clf.fit(global_feature.reshape(1,-1))[0]



print("Prediction: {}".format(prediction))
fh = open('prediction.json', 'w')
json.dump(prediction, fh)